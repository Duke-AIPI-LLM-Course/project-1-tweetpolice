{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunak/.virtualenvs/llm_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dataset path.\n",
    "model_path = \"distilbert/distilgpt2\"\n",
    "data_path = r\"../data/processed_data/train.json\"\n",
    "output_path = r\"../output\"\n",
    "\n",
    "# force to use GPU.\n",
    "assert torch.cuda.is_available(), \"Use GPU!\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a custom callback to record the change of loss value\n",
    "# in real time during model training.\n",
    "class LossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tokenizer):\n",
    "    dataset = load_dataset(\"json\", data_files=data_path, split=\"train[:1500]\")\n",
    "    print(dataset)\n",
    "\n",
    "    def format_example(example):\n",
    "        instruction = f\"Question: {example['Question']}\\nAnalysis: {example['Complex_CoT']}\"\n",
    "        inputs = tokenizer(\n",
    "            f\"{instruction}\\n### Answer: \\n{example['Response']}<|endoftext|>\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\"input_ids\": inputs[\"input_ids\"].squeeze(0), \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)}\n",
    "\n",
    "    return dataset.map(format_example, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration.\n",
    "# r means the rank of the low-rank decomposition.\n",
    "# lora_alpha is the scaling factor.\n",
    "# target_modules are the modules to be decomposed.\n",
    "# lora_dropout is the dropout rate.\n",
    "# task_type is the task type -- Causal Language Model.\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    per_device_train_batch_size=2,  # storage limited.\n",
    "    gradient_accumulation_steps=4,  # accumulate gradient, batch_size=8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,  # open fp16, accelerate training.\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    no_cuda=False,\n",
    "    dataloader_pin_memory=False,  # use pinned memory to accelerate training.\n",
    "    remove_unused_columns=False  # prevent error.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = {}\n",
    "for name, param in model.named_parameters():\n",
    "    params_before[name] = param.detach().cpu().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 82,723,584 || trainable%: 0.9804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Question', 'Complex_CoT', 'Response'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Start training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [561/561 02:05, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.281700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../output\n",
      "Loss curve saved to loss_curve.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # create output path.\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # load tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # load model.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # preprocess data.\n",
    "    dataset = process_data(tokenizer)\n",
    "\n",
    "    # loss callback.\n",
    "    loss_callback = LossCallback()\n",
    "\n",
    "    # data collator.\n",
    "    def data_collator(data):\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([torch.tensor(d[\"input_ids\"]) for d in data]).to(device),\n",
    "            \"attention_mask\": torch.stack([torch.tensor(d[\"attention_mask\"]) for d in data]).to(device),\n",
    "            # use input_ids as labels.\n",
    "            \"labels\": torch.stack([torch.tensor(d[\"input_ids\"]) for d in data]).to(device)\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    # create trainer.\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[loss_callback]\n",
    "    )\n",
    "\n",
    "    # start training.\n",
    "    print(\"Start training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # save model.\n",
    "    trainer.model.save_pretrained(output_path)\n",
    "    print(f\"Model saved to {output_path}\")\n",
    "\n",
    "    # plot loss curve.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_callback.losses)\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(os.path.join(output_path, \"loss_curve.png\"))\n",
    "    print(\"Loss curve saved to loss_curve.png\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_after = {}\n",
    "for name, param in trained_model.named_parameters():\n",
    "    params_after[name] = param.detach().cpu().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_before.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.0.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.0.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.0.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.0.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.0.mlp.c_proj.bias not present in params after training!\n",
      "transformer.h.1.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.1.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.1.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.1.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.1.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.1.mlp.c_proj.bias not present in params after training!\n",
      "transformer.h.2.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.2.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.2.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.2.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.2.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.2.mlp.c_proj.bias not present in params after training!\n",
      "transformer.h.3.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.3.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.3.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.3.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.3.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.3.mlp.c_proj.bias not present in params after training!\n",
      "transformer.h.4.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.4.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.4.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.4.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.4.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.4.mlp.c_proj.bias not present in params after training!\n",
      "transformer.h.5.attn.c_attn.weight not present in params after training!\n",
      "transformer.h.5.attn.c_attn.bias not present in params after training!\n",
      "transformer.h.5.attn.c_proj.weight not present in params after training!\n",
      "transformer.h.5.attn.c_proj.bias not present in params after training!\n",
      "transformer.h.5.mlp.c_proj.weight not present in params after training!\n",
      "transformer.h.5.mlp.c_proj.bias not present in params after training!\n"
     ]
    }
   ],
   "source": [
    "for key in params_before.keys():\n",
    "    transformed_key = \"base_model.model.\" + key\n",
    "    if transformed_key not in params_after:\n",
    "        print(f\"{key} not present in params after training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_model.model.transformer.wte.weight', 'base_model.model.transformer.wpe.weight', 'base_model.model.transformer.h.0.ln_1.weight', 'base_model.model.transformer.h.0.ln_1.bias', 'base_model.model.transformer.h.0.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.0.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.0.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.ln_2.weight', 'base_model.model.transformer.h.0.ln_2.bias', 'base_model.model.transformer.h.0.mlp.c_fc.weight', 'base_model.model.transformer.h.0.mlp.c_fc.bias', 'base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.0.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.ln_1.weight', 'base_model.model.transformer.h.1.ln_1.bias', 'base_model.model.transformer.h.1.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.1.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.1.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.ln_2.weight', 'base_model.model.transformer.h.1.ln_2.bias', 'base_model.model.transformer.h.1.mlp.c_fc.weight', 'base_model.model.transformer.h.1.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.1.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.ln_1.weight', 'base_model.model.transformer.h.2.ln_1.bias', 'base_model.model.transformer.h.2.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.2.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.2.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.ln_2.weight', 'base_model.model.transformer.h.2.ln_2.bias', 'base_model.model.transformer.h.2.mlp.c_fc.weight', 'base_model.model.transformer.h.2.mlp.c_fc.bias', 'base_model.model.transformer.h.2.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.2.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.ln_1.weight', 'base_model.model.transformer.h.3.ln_1.bias', 'base_model.model.transformer.h.3.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.3.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.3.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.ln_2.weight', 'base_model.model.transformer.h.3.ln_2.bias', 'base_model.model.transformer.h.3.mlp.c_fc.weight', 'base_model.model.transformer.h.3.mlp.c_fc.bias', 'base_model.model.transformer.h.3.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.3.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.ln_1.weight', 'base_model.model.transformer.h.4.ln_1.bias', 'base_model.model.transformer.h.4.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.4.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.4.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.ln_2.weight', 'base_model.model.transformer.h.4.ln_2.bias', 'base_model.model.transformer.h.4.mlp.c_fc.weight', 'base_model.model.transformer.h.4.mlp.c_fc.bias', 'base_model.model.transformer.h.4.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.4.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.ln_1.weight', 'base_model.model.transformer.h.5.ln_1.bias', 'base_model.model.transformer.h.5.attn.c_attn.base_layer.weight', 'base_model.model.transformer.h.5.attn.c_attn.base_layer.bias', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.base_layer.weight', 'base_model.model.transformer.h.5.attn.c_proj.base_layer.bias', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.ln_2.weight', 'base_model.model.transformer.h.5.ln_2.bias', 'base_model.model.transformer.h.5.mlp.c_fc.weight', 'base_model.model.transformer.h.5.mlp.c_fc.bias', 'base_model.model.transformer.h.5.mlp.c_proj.base_layer.weight', 'base_model.model.transformer.h.5.mlp.c_proj.base_layer.bias', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.ln_f.weight', 'base_model.model.transformer.ln_f.bias'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_after.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(trained_model.named_modules())[\"base_model.model.transformer.h.0.attn.c_attn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3895e-03, -2.3328e-01,  1.1973e+00,  ...,  1.3220e-01,\n",
      "          -2.4646e-01,  2.7246e-01],\n",
      "         [ 1.6455e-01, -2.1543e+00, -1.0022e-01,  ...,  2.0691e-02,\n",
      "           1.2549e-01,  2.7759e-01],\n",
      "         [-1.0651e-01, -2.6758e-01, -5.6787e-01,  ..., -9.2224e-02,\n",
      "          -2.0923e-01,  2.6782e-01],\n",
      "         [ 4.7632e-01, -1.0273e+00, -6.9580e-01,  ...,  1.4709e-01,\n",
      "           1.9226e-01,  1.5137e-01],\n",
      "         [-1.2286e-01, -1.8037e+00, -1.9922e+00,  ..., -7.2693e-02,\n",
      "           5.0995e-02,  3.1470e-01],\n",
      "         [-1.3857e+00, -2.1113e+00,  5.8105e-01,  ..., -6.9946e-02,\n",
      "           1.7041e-01,  6.5674e-02]]], device='cuda:0', dtype=torch.float16)\n",
      "tensor([[[-1.6642e-03, -2.4243e-01,  1.2021e+00,  ...,  1.2805e-01,\n",
      "          -2.4084e-01,  2.7515e-01],\n",
      "         [ 1.6370e-01, -2.1406e+00, -9.8328e-02,  ...,  2.8687e-02,\n",
      "           1.2622e-01,  2.8149e-01],\n",
      "         [-1.0724e-01, -2.7490e-01, -5.6250e-01,  ..., -9.0088e-02,\n",
      "          -2.1265e-01,  2.6733e-01],\n",
      "         [ 4.7485e-01, -1.0264e+00, -6.9727e-01,  ...,  1.5295e-01,\n",
      "           1.9116e-01,  1.5393e-01],\n",
      "         [-1.2341e-01, -1.7939e+00, -1.9951e+00,  ..., -6.8115e-02,\n",
      "           4.6661e-02,  3.1372e-01],\n",
      "         [-1.3857e+00, -2.1133e+00,  5.8350e-01,  ..., -6.7444e-02,\n",
      "           1.7456e-01,  7.0496e-02]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Dummy input\n",
    "input_text = \"Hello, how are you?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass through the attention layer\n",
    "with torch.no_grad():\n",
    "    inputs = trained_model.model.transformer.wte(inputs.input_ids)\n",
    "    c_attn = trained_model.model.transformer.h[0].attn.c_attn\n",
    "    conv = c_attn.base_layer\n",
    "    A = c_attn.lora_A[\"default\"].weight\n",
    "    B = c_attn.lora_B[\"default\"].weight\n",
    "\n",
    "    mult = (B @ A).T\n",
    "    mult = mult.to(torch.float16)\n",
    "    mult = inputs @ mult\n",
    "    custom_output = conv(inputs) + mult\n",
    "    lora_output = trained_model.model.transformer.h[0].attn.c_attn(\n",
    "        inputs\n",
    "    )\n",
    "    print(custom_output)\n",
    "\n",
    "print(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 5.9662e-03, -6.4502e-01,  8.1348e-01,  ..., -4.4739e-02,\n",
      "          -1.0846e-01,  1.1938e-01],\n",
      "         [-2.1094e-01, -7.8223e-01,  9.7852e-01,  ...,  8.9874e-03,\n",
      "          -6.7139e-02,  2.0386e-01],\n",
      "         [-1.8005e-01,  1.1865e+00,  2.8491e-01,  ...,  1.8282e-03,\n",
      "          -4.8492e-02,  1.3892e-01],\n",
      "         [-9.4043e-01,  7.9834e-01,  6.4600e-01,  ...,  4.9988e-02,\n",
      "          -4.3640e-02,  1.4905e-01],\n",
      "         [-1.6602e+00,  1.9355e+00,  5.1855e-01,  ...,  2.6077e-02,\n",
      "           4.5288e-02,  8.8745e-02],\n",
      "         [-1.7004e-01,  9.4775e-01,  7.8906e-01,  ...,  6.8176e-02,\n",
      "          -1.2231e-01,  1.2903e-01]]], device='cuda:0', dtype=torch.float16), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, how are you?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass through the attention layer\n",
    "with torch.no_grad():\n",
    "    output = model.transformer.h[0].attn(\n",
    "        model.transformer.wte(inputs.input_ids)\n",
    "    )\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6519,  0.2373, -1.3818,  ..., -0.2002,  0.1562, -0.0157],\n",
       "         [ 0.0896,  0.1294, -0.2100,  ...,  0.0294,  0.0270, -0.0498],\n",
       "         [ 0.2969, -0.1543, -0.3987,  ...,  0.0509,  0.0052, -0.0853],\n",
       "         [-0.4858, -1.6016, -0.2412,  ..., -0.0413, -0.0234,  0.2356],\n",
       "         [ 0.3691,  1.9355,  0.5186,  ..., -0.0152,  0.0482,  0.0887],\n",
       "         [-1.4512,  0.2700,  0.7891,  ...,  0.0440, -0.0495,  0.0829]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output[0] - lora_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x79338ce8edc0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.transformer.h.0.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.0.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.0.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.1.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.1.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.1.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.2.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.2.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.2.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.3.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.3.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.3.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.4.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.4.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.4.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.5.attn.c_attn\n",
      "Weight corresponding to LORA :  torch.Size([768, 2304])\n",
      "Updated w shape :  torch.Size([768, 2304])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([2304, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.5.attn.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([768, 768])\n",
      "Updated w shape :  torch.Size([768, 768])\n",
      "LoRA A: torch.Size([16, 768])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n",
      "Layer: base_model.model.transformer.h.5.mlp.c_proj\n",
      "Weight corresponding to LORA :  torch.Size([3072, 768])\n",
      "Updated w shape :  torch.Size([3072, 768])\n",
      "LoRA A: torch.Size([16, 3072])\n",
      "LoRA B: torch.Size([768, 16])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, module in trained_model.named_modules():\n",
    "    if \"lora_A\" in dict(module.named_modules()) and \"base_layer\" in dict(module.named_modules()):\n",
    "        print(f\"Layer: {name}\")\n",
    "        base_weight = module.base_layer.weight\n",
    "        print(\"Weight corresponding to LORA : \", base_weight.shape)\n",
    "        lora_A = module.lora_A[\"default\"].weight\n",
    "        lora_B = module.lora_B[\"default\"].weight\n",
    "        updated_W = base_weight + (lora_B @ lora_A).T\n",
    "        print(\"Updated w shape : \", updated_W.shape)\n",
    "        print(\"LoRA A:\", module.lora_A[\"default\"].weight.shape)\n",
    "        print(\"LoRA B:\", module.lora_B[\"default\"].weight.shape)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
