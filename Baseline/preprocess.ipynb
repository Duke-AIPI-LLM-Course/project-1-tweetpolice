{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by:\n",
    "      - Converting to lowercase\n",
    "      - Removing URLs, mentions, hashtags, punctuation, and extra spaces\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (you may choose to keep the word; here we remove the '#' symbol)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_lm(row):\n",
    "    \"\"\"\n",
    "    Format a row for language model fine-tuning.\n",
    "    Creates a prompt-response pair suitable for instruction fine-tuning.\n",
    "    Includes a 3-dimensional vector representing the three classification labels.\n",
    "    \"\"\"\n",
    "    # Get the class label\n",
    "    class_label = row['class']\n",
    "    \n",
    "    # Create the instruction/prompt\n",
    "    question = f\"{row['tweet']}\"\n",
    "    \n",
    "    # Create a 3-dimensional vector representing the labels\n",
    "    # Normalize the counts to create a probability distribution\n",
    "    total_annotations = row['hate_speech'] + row['offensive_language'] + row['neither']\n",
    "    if total_annotations > 0:  # Avoid division by zero\n",
    "        label_vector = [\n",
    "            float(row['hate_speech']) / total_annotations,\n",
    "            float(row['offensive_language']) / total_annotations,\n",
    "            float(row['neither']) / total_annotations\n",
    "        ]\n",
    "    else:\n",
    "        label_vector = [0.0, 0.0, 0.0]\n",
    "    \n",
    "    return {\n",
    "        \"Question\": question,\n",
    "        \"Label\": label_vector\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_csv, output_dir, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the CSV file; adjust quoting if necessary\n",
    "    df = pd.read_csv(input_csv, quotechar='\"')\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['tweet', 'class', 'hate_speech', 'offensive_language', 'neither']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' is missing from the dataset.\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    df = df.dropna(subset=required_columns)\n",
    "    \n",
    "    # Clean the tweet text\n",
    "    df['clean_tweet'] = df['tweet'].apply(clean_text)\n",
    "    print(df)\n",
    "    \n",
    "    # Print the class distribution\n",
    "    print(\"Class distribution:\")\n",
    "    print(df['class'].value_counts())\n",
    "    \n",
    "    # Split the data into training+validation and test sets\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, stratify=df['class'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Further split training+validation into training and validation sets.\n",
    "    # Calculate relative validation size from the remaining data.\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_relative_size, stratify=train_val_df['class'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Format data for language model fine-tuning\n",
    "    train_formatted = [format_for_lm(row) for _, row in train_df.iterrows()]\n",
    "    val_formatted = [format_for_lm(row) for _, row in val_df.iterrows()]\n",
    "    test_formatted = [format_for_lm(row) for _, row in test_df.iterrows()]\n",
    "    \n",
    "    # Save the formatted data as JSON files\n",
    "    train_json_path = os.path.join(output_dir, \"train.json\")\n",
    "    val_json_path = os.path.join(output_dir, \"val.json\")\n",
    "    test_json_path = os.path.join(output_dir, \"test.json\")\n",
    "    \n",
    "    with open(train_json_path, 'w') as f:\n",
    "        json.dump(train_formatted, f, indent=2)\n",
    "    \n",
    "    with open(val_json_path, 'w') as f:\n",
    "        json.dump(val_formatted, f, indent=2)\n",
    "    \n",
    "    with open(test_json_path, 'w') as f:\n",
    "        json.dump(test_formatted, f, indent=2)\n",
    "    \n",
    "    # Also save a single JSONL file for easy loading with datasets library\n",
    "    train_jsonl_path = os.path.join(output_dir, \"train.jsonl\")\n",
    "    with open(train_jsonl_path, 'w') as f:\n",
    "        for item in train_formatted:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"Training data saved to {train_json_path} and {train_jsonl_path} ({len(train_formatted)} samples)\")\n",
    "    print(f\"Validation data saved to {val_json_path} ({len(val_formatted)} samples)\")\n",
    "    print(f\"Test data saved to {test_json_path} ({len(test_formatted)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \\\n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "...                                                  ...   \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "24779  you've gone and broke the wrong heart baby, an...   \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "24781              youu got wild bitches tellin you lies   \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "                                             clean_tweet  \n",
      "0      rt as a woman you shouldnt complain about clea...  \n",
      "1      rt boy dats coldtyga dwn bad for cuffin dat ho...  \n",
      "2      rt dawg rt you ever fuck a bitch and she start...  \n",
      "3                              rt she look like a tranny  \n",
      "4      rt the shit you hear about me might be true or...  \n",
      "...                                                  ...  \n",
      "24778  yous a muthafin lie 8220 right his tl is trash...  \n",
      "24779  youve gone and broke the wrong heart baby and ...  \n",
      "24780  young buck wanna eat dat nigguh like i aint fu...  \n",
      "24781              youu got wild bitches tellin you lies  \n",
      "24782  ruffled ntac eileen dahlia beautiful color com...  \n",
      "\n",
      "[24783 rows x 8 columns]\n",
      "Class distribution:\n",
      "class\n",
      "1    19190\n",
      "2     4163\n",
      "0     1430\n",
      "Name: count, dtype: int64\n",
      "Training data saved to ../data/processed_data/train.json and ../data/processed_data/train.jsonl (17347 samples)\n",
      "Validation data saved to ../data/processed_data/val.json (2479 samples)\n",
      "Test data saved to ../data/processed_data/test.json (4957 samples)\n"
     ]
    }
   ],
   "source": [
    "input_csv=\"../data/labeled_data.csv\"\n",
    "output_dir=\"../data/processed_data/\"\n",
    "test_size=0.2\n",
    "val_size=0.1\n",
    "random_state=42\n",
    "\n",
    "process_data(\n",
    "    input_csv=input_csv,\n",
    "    output_dir=output_dir,\n",
    "    test_size=test_size,\n",
    "    val_size=val_size,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
